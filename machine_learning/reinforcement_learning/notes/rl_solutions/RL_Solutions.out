\BOOKMARK [1][-]{section.1}{Dynamic Programing}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Policy evaluation}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Policy improvement}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Policy iteration}{section.1}% 4
\BOOKMARK [3][-]{subsubsection.1.3.1}{Value iteration}{subsection.1.3}% 5
\BOOKMARK [3][-]{subsubsection.1.3.2}{Asynchronous DP}{subsection.1.3}% 6
\BOOKMARK [2][-]{subsection.1.4}{Generalized policy iteration}{section.1}% 7
\BOOKMARK [1][-]{section.2}{Monte Carlo Methods}{}% 8
\BOOKMARK [2][-]{subsection.2.1}{Monte Carlo policy evaluation}{section.2}% 9
\BOOKMARK [3][-]{subsubsection.2.1.1}{Vocabulary}{subsection.2.1}% 10
\BOOKMARK [3][-]{subsubsection.2.1.2}{The FVMC algorithm}{subsection.2.1}% 11
\BOOKMARK [3][-]{subsubsection.2.1.3}{Monte Carlo estimation of action values}{subsection.2.1}% 12
\BOOKMARK [2][-]{subsection.2.2}{Monte Carlo Control}{section.2}% 13
\BOOKMARK [3][-]{subsubsection.2.2.1}{On Policy Monte Carlo Control}{subsection.2.2}% 14
\BOOKMARK [3][-]{subsubsection.2.2.2}{Evaluation a policy while following another}{subsection.2.2}% 15
\BOOKMARK [3][-]{subsubsection.2.2.3}{Off-Policy Monte Carlo Control}{subsection.2.2}% 16
\BOOKMARK [2][-]{subsection.2.3}{Brief Summary}{section.2}% 17
\BOOKMARK [1][-]{section.3}{Temporal Differences Methods}{}% 18
\BOOKMARK [2][-]{subsection.3.1}{Temporal differences policy evaluation}{section.3}% 19
\BOOKMARK [3][-]{subsubsection.3.1.1}{The prediction problem with TD}{subsection.3.1}% 20
\BOOKMARK [3][-]{subsubsection.3.1.2}{The advantages of TD prediction method}{subsection.3.1}% 21
\BOOKMARK [2][-]{subsection.3.2}{Temporal Difference Control}{section.3}% 22
\BOOKMARK [3][-]{subsubsection.3.2.1}{SARSA : On-Policy TD Control}{subsection.3.2}% 23
\BOOKMARK [3][-]{subsubsection.3.2.2}{Q-learning : Off-Policy TD Control}{subsection.3.2}% 24
\BOOKMARK [2][-]{subsection.3.3}{Brief Summary}{section.3}% 25
