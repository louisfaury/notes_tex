\BOOKMARK [1][-]{section.1}{Regularization}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Parameter norm penalty}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Noise robustness}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Early stopping}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Dropout}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Others}{section.1}% 6
\BOOKMARK [1][-]{section.2}{Optimization}{}% 7
\BOOKMARK [2][-]{subsection.2.1}{Minibatch optimization}{section.2}% 8
\BOOKMARK [2][-]{subsection.2.2}{Challenges in neural network optimization}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.3}{Basic algorithms}{section.2}% 10
\BOOKMARK [3][-]{subsubsection.2.3.1}{Stochastic gradient descent}{subsection.2.3}% 11
\BOOKMARK [3][-]{subsubsection.2.3.2}{Momentum}{subsection.2.3}% 12
\BOOKMARK [3][-]{subsubsection.2.3.3}{Nesterov momentum}{subsection.2.3}% 13
\BOOKMARK [2][-]{subsection.2.4}{Parameter initialization strategies}{section.2}% 14
\BOOKMARK [2][-]{subsection.2.5}{Adaptative learning rates algorithms}{section.2}% 15
\BOOKMARK [3][-]{subsubsection.2.5.1}{AdaGrad}{subsection.2.5}% 16
\BOOKMARK [3][-]{subsubsection.2.5.2}{RMSProp}{subsection.2.5}% 17
\BOOKMARK [3][-]{subsubsection.2.5.3}{Adam}{subsection.2.5}% 18
\BOOKMARK [2][-]{subsection.2.6}{Approximate second order methods}{section.2}% 19
